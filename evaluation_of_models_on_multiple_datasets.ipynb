{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_metric\n",
    "import pandas as pd\n",
    "from datasets import ClassLabel, Sequence\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import default_data_collator\n",
    "from datasets import Dataset\n",
    "import collections\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_v2 = False\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "batch_size = 16\n",
    "\n",
    "# done\n",
    "# m_name = 'squad_base'\n",
    "# saved_model = './saved_model_V5'\n",
    "\n",
    "# done\n",
    "# m_name = 'squad_supersampled_all_v1'\n",
    "# saved_model = '../../debiased_models/bert-base-uncased-finetuned-squad_with_callbacks_all_super'\n",
    "\n",
    "# done\n",
    "# m_name = 'squad_supersampled_all_v2'\n",
    "# saved_model = '../../debiased_models/bert-base-uncased-finetuned-squad_with_callbacks_all_super_v2'\n",
    "\n",
    "# done\n",
    "# m_name = 'squad_supersampled_all_dd_ns'\n",
    "# saved_model = '../../debiased_models/bert-base-uncased-finetuned-squad_with_callbacks_all_super_dd_ns'\n",
    "\n",
    "# done -\n",
    "# m_name = 'squad_supersampled_similar_words_4'\n",
    "# saved_model = '../../debiased_models/bert-base-uncased-finetuned-squad_with_callbacks_reduced_on_similar_words_4_super'\n",
    "\n",
    "# done -\n",
    "# m_name = 'squad_supersampled_answer_length_3'\n",
    "# saved_model = '../../debiased_models/bert-base-uncased-finetuned-squad_with_callbacks_reduced_on_answer_length_3_super'\n",
    "\n",
    "m_name = 'squad_supersampled_distances_7'\n",
    "saved_model = '../../debiased_models/bert-base-uncased-finetuned-squad_with_callbacks_reduced_on_distances_7_super'\n",
    "\n",
    "# done\n",
    "# m_name = 'squad_supersampled_ans_sub_pos_1'\n",
    "# saved_model = '../../debiased_models/bert-base-uncased-finetuned-squad_with_callbacks_reduced_on_ans_sub_pos_1_super'\n",
    "\n",
    "# done\n",
    "# m_name = 'squad_supersampled_cosine_similarity_01'\n",
    "# saved_model = '../../debiased_models/bert-base-uncased-finetuned-squad_with_callbacks_reduced_on_cosine_similarity_01_super'\n",
    "\n",
    "# done\n",
    "# m_name = 'squad_supersampled_similar_entities_0'\n",
    "# saved_model = '../../debiased_models/bert-base-uncased-finetuned-squad_with_callbacks_reduced_on_sim_ents_0_super'\n",
    "\n",
    "\n",
    "\n",
    "# saved_model = './bert-base-uncased-finetuned-squad_with_callbacks_reduced_on_answer_length_3'\n",
    "# saved_model = './bert-base-uncased-finetuned-squad_with_callbacks_reduced_on_similar_words_4'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ../../debiased_models/bert-base-uncased-finetuned-squad_with_callbacks_reduced_on_distances_7_super/added_tokens.json. We won't load it.\n",
      "loading file ../../debiased_models/bert-base-uncased-finetuned-squad_with_callbacks_reduced_on_distances_7_super/vocab.txt\n",
      "loading file ../../debiased_models/bert-base-uncased-finetuned-squad_with_callbacks_reduced_on_distances_7_super/tokenizer.json\n",
      "loading file None\n",
      "loading file ../../debiased_models/bert-base-uncased-finetuned-squad_with_callbacks_reduced_on_distances_7_super/special_tokens_map.json\n",
      "loading file ../../debiased_models/bert-base-uncased-finetuned-squad_with_callbacks_reduced_on_distances_7_super/tokenizer_config.json\n",
      "loading configuration file ../../debiased_models/bert-base-uncased-finetuned-squad_with_callbacks_reduced_on_distances_7_super/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../../debiased_models/bert-base-uncased-finetuned-squad_with_callbacks_reduced_on_distances_7_super\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ../../debiased_models/bert-base-uncased-finetuned-squad_with_callbacks_reduced_on_distances_7_super/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "All the weights of BertForQuestionAnswering were initialized from the model checkpoint at ../../debiased_models/bert-base-uncased-finetuned-squad_with_callbacks_reduced_on_distances_7_super.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(saved_model)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(saved_model)\n",
    "data_collator = default_data_collator\n",
    "metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "max_length = 384 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-squad_with_callbacks\",\n",
    "#     evaluation_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps = 500, # Evaluation and Save happens every 10 steps\n",
    "    save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "#     metric_for_best_model = 'f1',\n",
    "#     push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    # train_dataset=tokenized_datasets[\"train\"],\n",
    "    # eval_dataset=dataset1[\"train\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,    \n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=8)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # Build a map example to its corresponding features.\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # The dictionaries we have to fill.\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # Logging.\n",
    "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "    # Let's loop over all the examples!\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # Those are the indices of the features associated to the current example.\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # Only used if squad_v2 is True.\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # Looping through all the features associated to the current example.\n",
    "        for feature_index in feature_indices:\n",
    "            # We grab the predictions of the model for this feature.\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
    "            # context.\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # Update minimum null prediction.\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            \n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "                    # to part of the input_ids that are not in the context.\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "                    if len(offset_mapping[start_index]) == 0 or len(offset_mapping[end_index]) == 0: \n",
    "                        continue\n",
    "                                        \n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "            # failure.\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation_on_dataset(dataset, save_dataframe_with_predictions = False, name = 'model_name'):\n",
    "    validation_features = dataset.map(\n",
    "        prepare_validation_features,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    raw_predictions = trainer.predict(validation_features)\n",
    "    \n",
    "    validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))\n",
    "\n",
    "    final_predictions = postprocess_qa_predictions(dataset, validation_features, raw_predictions.predictions)\n",
    "\n",
    "    predictions = [v for k, v in final_predictions.items()]\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in dataset]\n",
    "    \n",
    "    if save_dataframe_with_predictions:\n",
    "        data = pd.DataFrame(dataset)   \n",
    "        data['prediction_text'] = predictions\n",
    "        data.to_json('./from_debiased_models/' + name + '.json', orient='records')\n",
    "\n",
    "    return metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of models on datasets\n",
    "\n",
    "#### SQuAD dataset \n",
    "\n",
    "| model | exact match | F1 |\n",
    "| ----- | ----------- | -- |\n",
    "| model V5 | 80.19867549668874 | 87.57572428424854 |\n",
    "| model similar words 4 super | 78.50520340586566 | 86.6239696683917 |\n",
    "| model asnwer length 3 super | 77.97540208136235 | 86.58291287414434 |\n",
    "| model distances 7 super | 78.74172185430463 | 86.6761707572857 |\n",
    "\n",
    "#### AdversarialQA dataset\n",
    "\n",
    "| model | exact match | F1 |\n",
    "| ----- | ----------- | -- |\n",
    "| model V5 | 19.866666666666667 | 31.299247172832363 |\n",
    "| model similar words 4 super | 18.7 | 30.726865505717093 |\n",
    "| model asnwer length 3 super | 17.533333333333335 | 29.53898631645027 |\n",
    "| model distances 7 super | 18.5 | 30.25014973765301 |\n",
    "\n",
    "#### Natural Questions dataset\n",
    "\n",
    "| model | exact match | F1 |\n",
    "| ----- | ----------- | -- |\n",
    "| model V5 | 55.730050933786075 | 67.82058427738443 |\n",
    "| model similar words 4 super | 56.49405772495756 | 69.36010918865499 |\n",
    "| model asnwer length 3 super | 50.67911714770798 | 64.46202025268495 |\n",
    "| model distances 7 super | 58.31918505942275 | 70.33259655249557 |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQuAD dataset \n",
    "\n",
    "model V5 {'exact_match': 80.19867549668874, 'f1': 87.57572428424854}\n",
    "\n",
    "model similar words 4 {'exact_match': 78.64711447492904, 'f1': 86.74478847823545}\n",
    "\n",
    "model asnwer length 3 {'exact_match': 76.72658467360453, 'f1': 85.70742317417624}\n",
    "\n",
    "model similar words 4 super {'exact_match': 78.50520340586566, 'f1': 86.6239696683917}\n",
    "\n",
    "model asnwer length 3 super {'exact_match': 77.97540208136235, 'f1': 86.58291287414434}\n",
    "\n",
    "model distances 7 super {'exact_match': 78.74172185430463, 'f1': 86.6761707572857}\n",
    "\n",
    "model all super {'exact_match': 78.83632923368023, 'f1': 86.8641872979563}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/luki/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7689a81459bd4067b2c1db1f7c2388f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/luki/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-caf3761ed2a4f892.arrow\n",
      "The following columns in the test set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10784\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3281' max='674' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [674/674 35:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 10570 example predictions split into 10784 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdad74020b7846c2b436e91739d4f676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 78.74172185430463, 'f1': 86.6761707572857}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = model_evaluation_on_dataset(datasets['validation']) #, save_dataframe_with_predictions = True, name = m_name)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"evaluation_of_models_on_datasets.csv\", \"a\") as file_append:\n",
    "        file_append.write(f\"\\nSQuAD,{m_name},{metrics['exact_match']},{metrics['exact_match']-80.19867549668874},{metrics['f1']},{metrics['f1']-87.57572428424854}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdversarialQA dataset\n",
    "\n",
    "model V5 {'exact_match': 19.866666666666667, 'f1': 31.299247172832363}\n",
    "\n",
    "model similar words 4 {'exact_match': 19.666666666666668, 'f1': 30.30664106943778}\n",
    "\n",
    "model asnwer length 3 {'exact_match': 16.3, 'f1': 28.295573437070573}\n",
    "\n",
    "model similar words 4 super {'exact_match': 18.7, 'f1': 30.726865505717093}\n",
    "\n",
    "model asnwer length 3 super {'exact_match': 17.533333333333335, 'f1': 29.53898631645027}\n",
    "\n",
    "model distances 7 super {'exact_match': 18.5, 'f1': 30.25014973765301}\n",
    "\n",
    "model all super {'exact_match': 18.7, 'f1': 30.417641963819342}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset adversarial_qa (/home/luki/.cache/huggingface/datasets/adversarial_qa/adversarialQA/1.0.0/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e690b7addbb845aaa9e65bb020aa64e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = load_dataset(\"adversarial_qa\", \"adversarialQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(datasets['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json('adversarialQA_validation.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/luki/.cache/huggingface/datasets/adversarial_qa/adversarialQA/1.0.0/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b/cache-9fe180c187805fed.arrow\n",
      "The following columns in the test set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3010\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 3000 example predictions split into 3010 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f845cc67194ed0a6d706c5122a1107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 18.5, 'f1': 30.25014973765301}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = model_evaluation_on_dataset(datasets['validation'])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"evaluation_of_models_on_datasets.csv\", \"a\") as file_append:\n",
    "        file_append.write(f\"\\nAdversarialQA,{m_name},{metrics['exact_match']},{metrics['exact_match']-19.866666666666667},{metrics['f1']},{metrics['f1']-31.299247172832363}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Questions dataset\n",
    "\n",
    "model V5 {'exact_match': 55.730050933786075, 'f1': 67.82058427738443}\n",
    "\n",
    "model similar words 4 {'exact_match': 55.85738539898132, 'f1': 67.81840013298724}\n",
    "\n",
    "model asnwer length 3 {'exact_match': 52.12224108658744, 'f1': 66.15090360227799}\n",
    "\n",
    "model similar words 4 super {'exact_match': 56.49405772495756, 'f1': 69.36010918865499}\n",
    "\n",
    "model asnwer length 3 super {'exact_match': 50.67911714770798, 'f1': 64.46202025268495}\n",
    "\n",
    "model distances 7 super {'exact_match': 58.31918505942275, 'f1': 70.33259655249557}\n",
    "\n",
    "model all super {'exact_match': 54.07470288624788, 'f1': 66.93559162155793}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('nq_dev_formated.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6915606477668963328</td>\n",
       "      <td>In logical argument and mathematical proof, th...</td>\n",
       "      <td>what do the 3 dots mean in math</td>\n",
       "      <td>{'text': ['the therefore sign (∴) is generally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5004457603684974592</td>\n",
       "      <td>The Super Bowl 50 Halftime Show took place on ...</td>\n",
       "      <td>who is playing the halftime show at super bowl...</td>\n",
       "      <td>{'text': ['British rock group Coldplay[1] with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8494342737238168576</td>\n",
       "      <td>Matthew MacKendree \"Matt\" Lanter (born April 1...</td>\n",
       "      <td>star wars the clone wars anakin voice actor</td>\n",
       "      <td>{'text': ['Matthew MacKendree \"Matt\" Lanter'],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7217222058435937280</td>\n",
       "      <td>The forum is best known for its annual meeting...</td>\n",
       "      <td>where was the world economic forum held this year</td>\n",
       "      <td>{'text': ['Davos', 'Davos, a mountain resort i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5533906981191706624</td>\n",
       "      <td>Since 1947, there have been eight Chief Minist...</td>\n",
       "      <td>who was the first chief minister of west bengal</td>\n",
       "      <td>{'text': ['Prafulla Chandra Ghosh', 'Prafulla ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>-8747637469328088064</td>\n",
       "      <td>Queen Elizabeth II is the sovereign, and her h...</td>\n",
       "      <td>who is next in line to inherit the british throne</td>\n",
       "      <td>{'text': ['Charles, Prince of Wales', 'her eld...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>-3916198822947717632</td>\n",
       "      <td>The memory in flash drives is commonly enginee...</td>\n",
       "      <td>how many writes does a flash drive have</td>\n",
       "      <td>{'text': ['around 3,000-5,000 program-erase cy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>-3200873089876979712</td>\n",
       "      <td>Thirty-three amendments to the United States C...</td>\n",
       "      <td>how many ammendments to the constitution have ...</td>\n",
       "      <td>{'text': ['Thirty-three', 'Twenty-seven'], 'an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>-4445560055142980608</td>\n",
       "      <td>3H (atomic mass 7000301604928199000♠3.01604928...</td>\n",
       "      <td>what is the mass number of hydrogen isotope th...</td>\n",
       "      <td>{'text': ['3.01604928199(23) u', '700030160492...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2355</th>\n",
       "      <td>1747539378277902336</td>\n",
       "      <td>\"Smoke Gets in Your Eyes\" is a show tune writt...</td>\n",
       "      <td>who sang smoke gets in your eyes first</td>\n",
       "      <td>{'text': ['Gertrude Niesen', 'Tamara Drasin'],...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2356 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                            context  \\\n",
       "0     6915606477668963328  In logical argument and mathematical proof, th...   \n",
       "1    -5004457603684974592  The Super Bowl 50 Halftime Show took place on ...   \n",
       "2     8494342737238168576  Matthew MacKendree \"Matt\" Lanter (born April 1...   \n",
       "3     7217222058435937280  The forum is best known for its annual meeting...   \n",
       "4     5533906981191706624  Since 1947, there have been eight Chief Minist...   \n",
       "...                   ...                                                ...   \n",
       "2351 -8747637469328088064  Queen Elizabeth II is the sovereign, and her h...   \n",
       "2352 -3916198822947717632  The memory in flash drives is commonly enginee...   \n",
       "2353 -3200873089876979712  Thirty-three amendments to the United States C...   \n",
       "2354 -4445560055142980608  3H (atomic mass 7000301604928199000♠3.01604928...   \n",
       "2355  1747539378277902336  \"Smoke Gets in Your Eyes\" is a show tune writt...   \n",
       "\n",
       "                                               question  \\\n",
       "0                       what do the 3 dots mean in math   \n",
       "1     who is playing the halftime show at super bowl...   \n",
       "2           star wars the clone wars anakin voice actor   \n",
       "3     where was the world economic forum held this year   \n",
       "4       who was the first chief minister of west bengal   \n",
       "...                                                 ...   \n",
       "2351  who is next in line to inherit the british throne   \n",
       "2352            how many writes does a flash drive have   \n",
       "2353  how many ammendments to the constitution have ...   \n",
       "2354  what is the mass number of hydrogen isotope th...   \n",
       "2355             who sang smoke gets in your eyes first   \n",
       "\n",
       "                                                answers  \n",
       "0     {'text': ['the therefore sign (∴) is generally...  \n",
       "1     {'text': ['British rock group Coldplay[1] with...  \n",
       "2     {'text': ['Matthew MacKendree \"Matt\" Lanter'],...  \n",
       "3     {'text': ['Davos', 'Davos, a mountain resort i...  \n",
       "4     {'text': ['Prafulla Chandra Ghosh', 'Prafulla ...  \n",
       "...                                                 ...  \n",
       "2351  {'text': ['Charles, Prince of Wales', 'her eld...  \n",
       "2352  {'text': ['around 3,000-5,000 program-erase cy...  \n",
       "2353  {'text': ['Thirty-three', 'Twenty-seven'], 'an...  \n",
       "2354  {'text': ['3.01604928199(23) u', '700030160492...  \n",
       "2355  {'text': ['Gertrude Niesen', 'Tamara Drasin'],...  \n",
       "\n",
       "[2356 rows x 4 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b736abc1124c470db3be72bb974af3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2388\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 2356 example predictions split into 2388 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8676d43e4f54b2da5503ec141a35a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 58.31918505942275, 'f1': 70.33259655249557}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = model_evaluation_on_dataset(dataset)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"evaluation_of_models_on_datasets.csv\", \"a\") as file_append:\n",
    "        file_append.write(f\"\\nNaturalQuestions,{m_name},{metrics['exact_match']},{metrics['exact_match']-55.730050933786075},{metrics['f1']},{metrics['f1']-67.82058427738443}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TriviaQA dataset\n",
    "\n",
    "base {'exact_match': 37.580071174377224, 'f1': 46.881850685798064}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('triviaqa_dev_formated.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tc_49--Kiss_You_All_Over.txt</td>\n",
       "      <td>`` Kiss You All Over '' is a 1978 song perform...</td>\n",
       "      <td>Who had a 70s No 1 hit with Kiss You All Over?</td>\n",
       "      <td>{'text': ['Exile'], 'answer_start': [62]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tc_56--Kathleen_Ferrier.txt</td>\n",
       "      <td>Kathleen Mary Ferrier , CBE ( 22 April 1912 - ...</td>\n",
       "      <td>What claimed the life of singer Kathleen Ferrier?</td>\n",
       "      <td>{'text': ['Cancer'], 'answer_start': [2488]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tc_106--Lauren_Bacall.txt</td>\n",
       "      <td>Lauren Bacall ( , born Betty Joan Perske ; Sep...</td>\n",
       "      <td>Which actress was voted Miss Greenwich Village...</td>\n",
       "      <td>{'text': ['Bacall'], 'answer_start': [7]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tc_137--Michael_Jackson.txt</td>\n",
       "      <td>Michael Joseph Jackson ( August 29 , 1958 – Ju...</td>\n",
       "      <td>What was the name of Michael Jackson's autobio...</td>\n",
       "      <td>{'text': ['moonwalk'], 'answer_start': [1508]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tc_217--Tanzania.txt</td>\n",
       "      <td>Tanzania , This approximates the Kiswahili pro...</td>\n",
       "      <td>Which volcano in Tanzania is the highest mount...</td>\n",
       "      <td>{'text': ['Kilimanjaro'], 'answer_start': [478]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9830</th>\n",
       "      <td>qg_4604--National_Guard_of_the_United_States.txt</td>\n",
       "      <td>The National Guard of the United States , part...</td>\n",
       "      <td>With a motto of Always Ready, Always There, wh...</td>\n",
       "      <td>{'text': ['National Guard'], 'answer_start': [4]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9831</th>\n",
       "      <td>qg_4615--Whoville.txt</td>\n",
       "      <td>Whoville is a fictional town created by author...</td>\n",
       "      <td>Who tried to steal Christmas from the town of ...</td>\n",
       "      <td>{'text': ['Grinch'], 'answer_start': [161]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9832</th>\n",
       "      <td>qg_4628--Winter_Wonderland.txt</td>\n",
       "      <td>`` Winter Wonderland '' is a winter song , pop...</td>\n",
       "      <td>What is the name of the parson mentioned in th...</td>\n",
       "      <td>{'text': ['Brown'], 'answer_start': [1908]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9833</th>\n",
       "      <td>qg_4628--Frosty's_Winter_Wonderland.txt</td>\n",
       "      <td>Frosty 's Winter Wonderland is a 1976 animated...</td>\n",
       "      <td>What is the name of the parson mentioned in th...</td>\n",
       "      <td>{'text': ['Brown'], 'answer_start': [2032]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9834</th>\n",
       "      <td>qg_4656--Horseshoes.txt</td>\n",
       "      <td>Horseshoes is an outdoor game played between t...</td>\n",
       "      <td>In what outdoor sport, sanctioned by the NHPA,...</td>\n",
       "      <td>{'text': ['Horseshoes'], 'answer_start': [0]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9835 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    id  \\\n",
       "0                         tc_49--Kiss_You_All_Over.txt   \n",
       "1                          tc_56--Kathleen_Ferrier.txt   \n",
       "2                            tc_106--Lauren_Bacall.txt   \n",
       "3                          tc_137--Michael_Jackson.txt   \n",
       "4                                 tc_217--Tanzania.txt   \n",
       "...                                                ...   \n",
       "9830  qg_4604--National_Guard_of_the_United_States.txt   \n",
       "9831                             qg_4615--Whoville.txt   \n",
       "9832                    qg_4628--Winter_Wonderland.txt   \n",
       "9833           qg_4628--Frosty's_Winter_Wonderland.txt   \n",
       "9834                           qg_4656--Horseshoes.txt   \n",
       "\n",
       "                                                context  \\\n",
       "0     `` Kiss You All Over '' is a 1978 song perform...   \n",
       "1     Kathleen Mary Ferrier , CBE ( 22 April 1912 - ...   \n",
       "2     Lauren Bacall ( , born Betty Joan Perske ; Sep...   \n",
       "3     Michael Joseph Jackson ( August 29 , 1958 – Ju...   \n",
       "4     Tanzania , This approximates the Kiswahili pro...   \n",
       "...                                                 ...   \n",
       "9830  The National Guard of the United States , part...   \n",
       "9831  Whoville is a fictional town created by author...   \n",
       "9832  `` Winter Wonderland '' is a winter song , pop...   \n",
       "9833  Frosty 's Winter Wonderland is a 1976 animated...   \n",
       "9834  Horseshoes is an outdoor game played between t...   \n",
       "\n",
       "                                               question  \\\n",
       "0        Who had a 70s No 1 hit with Kiss You All Over?   \n",
       "1     What claimed the life of singer Kathleen Ferrier?   \n",
       "2     Which actress was voted Miss Greenwich Village...   \n",
       "3     What was the name of Michael Jackson's autobio...   \n",
       "4     Which volcano in Tanzania is the highest mount...   \n",
       "...                                                 ...   \n",
       "9830  With a motto of Always Ready, Always There, wh...   \n",
       "9831  Who tried to steal Christmas from the town of ...   \n",
       "9832  What is the name of the parson mentioned in th...   \n",
       "9833  What is the name of the parson mentioned in th...   \n",
       "9834  In what outdoor sport, sanctioned by the NHPA,...   \n",
       "\n",
       "                                                answers  \n",
       "0             {'text': ['Exile'], 'answer_start': [62]}  \n",
       "1          {'text': ['Cancer'], 'answer_start': [2488]}  \n",
       "2             {'text': ['Bacall'], 'answer_start': [7]}  \n",
       "3        {'text': ['moonwalk'], 'answer_start': [1508]}  \n",
       "4      {'text': ['Kilimanjaro'], 'answer_start': [478]}  \n",
       "...                                                 ...  \n",
       "9830  {'text': ['National Guard'], 'answer_start': [4]}  \n",
       "9831        {'text': ['Grinch'], 'answer_start': [161]}  \n",
       "9832        {'text': ['Brown'], 'answer_start': [1908]}  \n",
       "9833        {'text': ['Brown'], 'answer_start': [2032]}  \n",
       "9834      {'text': ['Horseshoes'], 'answer_start': [0]}  \n",
       "\n",
       "[9835 rows x 4 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f40532e04034d94bc1044215e55f1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36286\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 9835 example predictions split into 36286 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54dd4d4fb0e466aa4f559d049338bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9835 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 36.86832740213523, 'f1': 46.59079620891165}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = model_evaluation_on_dataset(dataset)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"evaluation_of_models_on_datasets.csv\", \"a\") as file_append:\n",
    "        file_append.write(f\"\\nTriviaQA,{m_name},{metrics['exact_match']},{metrics['exact_match']-37.580071174377224},{metrics['f1']},{metrics['f1']-46.881850685798064}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUAC dataset\n",
    "\n",
    "model V5 {'exact_match': 6.966292134831461, 'f1': 28.27446363879073}\n",
    "\n",
    "model similar words 4 {'exact_match': 3.6128152692569873, 'f1': 16.698765837598508}\n",
    "\n",
    "model asnwer length 3 {'exact_match': 4.34560327198364, 'f1': 19.245582365309676}\n",
    "\n",
    "model asnwer length 3 super {'exact_match': 4.771642808452625, 'f1': 19.805298115366334}\n",
    "\n",
    "model distances 7 super {'exact_match': 4.0047716428084525, 'f1': 17.726421532100307}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_json('quac_dev_formated.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_evaluation_on_dataset(dataset)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7a7a7019c9e0c1b9f0e0c3be70dc9924c7adf8b61dfc45d9a6a9f2f7a665204"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
